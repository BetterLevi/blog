<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Why So Serious?</title>
  
    <link rel="icon" href="/blog/assets/smile.JPG">
  
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">

  
<link rel="stylesheet" href="/blog/css/style.css">

<meta name="generator" content="Hexo 5.3.0"></head>

<body>
<div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url(/blog/assets/header.png)">
        <div class='av-pic' style="background-image: url(/blog/assets/avatar.jpg)">
        </div>
    </section>
    <section class='menu'>
        <div>Why So Serious?</div>
        
        <ul>
          
            <a href="/blog/" class="Btn">
              <li>Home</li>
            </a>  
          
            <a href="/blog/categories/" class="Btn">
              <li>Categories</li>
            </a>  
          
            <a href="/blog/about/" class="Btn">
              <li>About</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <ul class="Index">
  
    <header class='PageTitle'>
        <h1>{ Hadoop }</h1>
    </header>
  
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/blog/2021/11/30/Hadoop%E4%B9%8BMapReduce%E4%BB%8B%E7%BB%8D/">Hadoop之MapReduce介绍</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2021-11-30T08:26:19.000Z" itemprop="datePublished">
    2021-11-30
  </time>
  
  
  / 
  <ul>
    
  <li class="meta-text">
  { <a href="/blog/categories/BigData/">BigData</a> }
  </li>

  <li class="meta-text">
  { <a href="/blog/categories/BigData/Hadoop/">Hadoop</a> }
  </li>


  </ul>
  
</div>

    </header>
    <div>
      
        <a id="more"></a>

<p>MapReduce计算模型将大数据任务分解为多个可在集群中并行执行的单个任务，通过合并单个任务的结果来得到最终的结果：</p>
<ul>
<li>Map：将输入计算为键值对的形式输出</li>
<li>Reduce：将相同的键发送到同一个Reduce任务上，计算这些键对应的值，最终输出一个键值对。</li>
</ul>
<h3 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h3><p>MapReduce体系结构主要由四个部分组成：Client，JobTracker(集群中只有一个)，TaskTracker，Task。</p>
<ol>
<li>Client：用户编写的MapReduce程序通过Client提交给JobTracker；用户通过Client提供的接口查询作业运行状态</li>
<li>JobTracker：负责资源监控和任务调度；监控所有的JobTracker和任务的健康状态，出现异常则将任务转移到其他节点；资源管理；</li>
<li>TaskTracker：周期性的将本节点上资源的使用情况和任务运行进度上报给JobTracker；处理JobTracker指令，如停止任务等</li>
<li>Task：分为Map Task和Reduce  Task，由TaskTracker启动</li>
</ol>
<h3 id="MapReduce执行过程"><a href="#MapReduce执行过程" class="headerlink" title="MapReduce执行过程"></a>MapReduce执行过程</h3><p>MapReduce执行时分为Mapper和Reducer两个阶段，Mapper阶段读取Hdfs中的数据文件，调用方法处理数据；Reducer阶段接收Mapper阶段输出的数据，调用方法进行计算，最后输出到Hdfs文件中。</p>
<h4 id="Mapper任务"><a href="#Mapper任务" class="headerlink" title="Mapper任务"></a>Mapper任务</h4><ol>
<li>把输入文件进行分片生成InputSplit，每个InputSplit大小固定。默认情况下InputSplit大小与block大小一致，每个InputSplit由一个Mapper进程处理。若block大小为64mb，输入两个文件大小分别为32mb，72mb，则对应输入三个block，每个block产生一个InputSplit，由三个Mapper进程处理。</li>
<li>对InputSplit中的数据按照一定的规则解析成键值对。如将每一行文本内容解析成键值对，键是每一行的起始位置，值是本行的文本内容。</li>
<li>调用Mapper类中的map方法。步骤2中解析出来的每一个键值对，调用一次map方法，输出零个或多个键值对。</li>
<li>按照规则对步骤3输出的键值对进行分区，分区基于键进行。默认只有一个分区，每个Reducer任务负责一个分区，即默认只有一个Reducer任务。</li>
<li>在每个分区中根据键值对进行排序，先按照键进行排序，键相同则按照值进行排序</li>
</ol>
<h4 id="Reduce任务"><a href="#Reduce任务" class="headerlink" title="Reduce任务"></a>Reduce任务</h4><ol>
<li>Reducer任务会主动从所有Mapper任务复制其输出的键值对</li>
<li>把复制到Reducer的本地数据，全部进行合并，即将分散的数据合并成一个大的数据集，然后再进行排序</li>
<li>对排序后的键值对调用reduce方法，键相同的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后将这些输出的键值对写入到Hdfs文件中。</li>
</ol>
<h4 id="Mapper数量"><a href="#Mapper数量" class="headerlink" title="Mapper数量"></a>Mapper数量</h4><p>由上可知，Mapper的数量由InputSplit数量决定，而InputSplit数量由输入的文件数量确定。Split是在block基础上进行逻辑切分，block时物理切分。</p>
<h3 id="MapReduce核心过程Shuffle"><a href="#MapReduce核心过程Shuffle" class="headerlink" title="MapReduce核心过程Shuffle"></a>MapReduce核心过程Shuffle</h3><p>Shuffle主要负责从map结束到reduce开始之间的过程。</p>
<h4 id="Map端shuffle"><a href="#Map端shuffle" class="headerlink" title="Map端shuffle"></a>Map端shuffle</h4><p>每个Mapper进程都有一个环形的内存缓冲区，用来存储map的输出数据，默认大小为100mb，当缓冲区占用达到0.8即80mb时，一个后台进程就会把数据溢写到磁盘中：首先按分区和键进行排序，排序结果为缓冲区内分区有序，同一个分区中的键有序。排序完成后会创建一个临时文件，然后启动一个线程将这部分数据spill到磁盘上。剩余的20%内存在此期间可以继续写入map输出的键值对。当一个map处理的数据很大超过缓冲区内存时，就会生成多个spill文件，会启动进程对同一个map任务产生的多个spill文件进行归并（merge）生成一个已经分区且排序的大文件。溢出写文件归并完成后，map将删除所有的临时溢出写文件，通知NodeManager任务完成，只要其中一个map task完成，reduce task就开始复制它的输出（按照分区号，每个Reducer读其对应的分区数据）</p>
<h4 id="Reduce端shuffle"><a href="#Reduce端shuffle" class="headerlink" title="Reduce端shuffle"></a>Reduce端shuffle</h4><ol>
<li><p>copy拉去数据<br>reduce进程启动copy线程，通过http方式请求map task所在的NodeManager来获取输出文件。NM需要为分区文件运行reduce任务，reduce读取可能对应多个map task，而每个map task执行进度不一致，因此当一个map task完成时，reduce任务便开始复制其输出。</p>
</li>
<li><p>merge合并小文件<br>Copy过来的数据会先放入内存缓冲区，Reducer会向每个map拉取数据，在内存中每个map对应一块数据，当内存缓冲区存储的map数据到达阈值时，开始把内存中的数据merge输出到磁盘文件中，即内存到磁盘merge。当该Reducer的map数据全部copy完成，reducer上会生成多个文件（若拉取的数据总量未超出内存大小，则无文件生成），执行磁盘合并操作，map输出的数据是有序的，merge进行一次合并排序（针对键进行归并排序，即reducer端的sort过程），最终reduce shuffle过程会输出一个整体有序的数据块。</p>
</li>
<li><p>reduce计算当Reduce任务完成全部的复制和排序后，会根据已排好序的key构造对应值的迭代器，默认根据键分组也可以自定义。对于默认分组，键相同则属于同一种，对应的value放入同一个迭代器，该迭代器的key与放入迭代器数据的key一致。</p>
<p>在reduce阶段，reduce方法的输入是所有的key和value迭代器，输出直接写到Hdfs。</p>
</li>
</ol>
<h3 id="Map-Join与Reduce-Join"><a href="#Map-Join与Reduce-Join" class="headerlink" title="Map Join与Reduce Join"></a>Map Join与Reduce Join</h3><p>map join是直接在map阶段完成数据的合并，没有reduce阶段；</p>
<p>reduce join是在map阶段完成数据的标记，在reduce阶段完成数据的合并。</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/blog/2021/11/11/Hadoop%E4%B9%8BHdfs/">Hadoop之Hdfs</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2021-11-11T06:29:13.000Z" itemprop="datePublished">
    2021-11-11
  </time>
  
  
  / 
  <ul>
    
  <li class="meta-text">
  { <a href="/blog/categories/BigData/">BigData</a> }
  </li>

  <li class="meta-text">
  { <a href="/blog/categories/BigData/Hadoop/">Hadoop</a> }
  </li>


  </ul>
  
</div>

    </header>
    <div>
      
        <a id="more"></a>

<h3 id="Hdfs中数据块block的设计"><a href="#Hdfs中数据块block的设计" class="headerlink" title="Hdfs中数据块block的设计"></a>Hdfs中数据块block的设计</h3><h4 id="block大小设置"><a href="#block大小设置" class="headerlink" title="block大小设置"></a>block大小设置</h4><p> Hdfs中的文件在物理上以块block的形式存储，块的大小可以通过参数进行配置(dfs.blocksize)，hadoop2.x版本中默认块大小为128M，之前版本为64M。</p>
<p>block大小设置的原则是最小化寻址开销，减少网络传输。</p>
<ul>
<li>设置合适的block大小有助于减少磁盘寻址时间，提高系统吞吐量；</li>
<li>NameNode需要在内存FSImage文件中记录DataNode中数据块信息，如果block size太小，需要维护的数据块信息增多，内存消耗增加；</li>
<li>若Map任务奔溃，重新启动加载数据是，block越大，数据加载时间越长，恢复任务慢；</li>
<li>主节点监控其他节点，每个节点会周期性的报告自己的工作状态，若节点超过时间阈值未报告则主节点视该节点下线，并将该节点的数据分发给其他节点。而时间阈值根据block size进行估算，若size设置不合理，容易误判节点死亡；</li>
<li>MapReduce中map任务通常一次只处理一个block中的数据，若block设置过大会导致任务数量太少导致运行速度过慢；</li>
<li>读写需要数据的网络传输，block过大网络传输时间过长导致程序超时无响应，任务执行过程中拉取其他节点的block或失败重试的成本太高；block过小则会频繁的进行文件传输，增加对网络和CPU的占用；</li>
</ul>
<h3 id="Hdfs架构"><a href="#Hdfs架构" class="headerlink" title="Hdfs架构"></a>Hdfs架构</h3><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><ul>
<li>负责文件元数据信息的操作以及处理客户端的请求；</li>
<li>管理Hdfs文件系统的命名空间NameSpace；</li>
<li>维护文件系统树FileSystem以及文件树中所有的文件和文件夹的元数据Metadata，维护文件到块的对应关系和block到节点的对应关系；</li>
<li>维护镜像文件fsimage和操作文件editlog，一般缓存在内存中也可持久化到磁盘；</li>
<li>记录文件中各个block所在的DataNode的位置信息，这些信息不会永久保存，在NameNode启动时，DataNode在向NameNode进行注册时进行缓存；</li>
</ul>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><ul>
<li>负责管理其节点上存储数据的读写，定期向NameNode发送心跳和文件块状态报告等 </li>
</ul>
<h4 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h4><p>NameNode启动时会生成整个文件系统的快照fsimage，启动后文件的改动信息记录在editlog(写过程由DataNode触发，当DataNode写文件操作后，与NameNode进行通信，告诉NameNode改文件的block信息，NameNode将这些信息保存在editlog中)，当NameNode重启时，editlog合并到fsimage中生成最新的系统快照。<br>          这种运行逻辑存在问题：</p>
<ol>
<li>NameNode长时间不重启，editlog会变得很大；</li>
<li>NameNode的重启会因为太大的editlog要合并到fsimage中而变得很慢；</li>
<li>NameNode挂掉，editlog丢失，而fsimage的版本太旧导致中间数据丢失。</li>
</ol>
<p>于是增加Secondary NameNode来解决上述问题，其职责为定时查询NameNode上的editlog并将其合并到自己的fsimage中，合并完成后将新的fsimage拷贝到NameNode上进行替换。</p>
<h3 id="HA设计"><a href="#HA设计" class="headerlink" title="HA设计"></a>HA设计</h3><p> Hadoop1.x 中只有一个NameNode，存在单点问题，即NameNode挂了后集群不可用。2.x中提供两种方式实现高可用：NFS(Network File System) 和 JN(JournalNode，Quorum Journal Manager)。</p>
<h4 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h4><p>集群中有两个NameNode，一个为Active NameNode，另一个为Standby NameNode，这两个NameNode状态可以切换，但是只能存在一个Active NameNode，且只有Active NameNode对外提供服务而Standby NameNode不提供服务。两种NameNode之间通过NFS或JN进行editlog同步。</p>
<p>Active NameNode更新editlog后将其上传NFS或者JN，Standby NameNode定时从NFS或JN上读取editlog，然后合并至fsimage上生成最新的fsimage，合并完成后通知Active NameNode获取新的fsimage进行替换。<br> 所以Active NameNode和Standby NameNode上的fsimage都是最新的，当Active NameNode挂了，可以直接将Standby NameNode切换为Active NameNode。</p>
<ul>
<li>NFS：一个共享的文件存储系统，Active NameNode写文件， Standby NameNode读文件，一旦不可用则数据无法继续同步。</li>
<li>JN(Quorum Journal Manager): JournalNode集群，由2N+1个节点组成，最多可容忍N个节点挂掉，保证editlog日志文件共享存储系统的可用性。</li>
</ul>
<h4 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h4><p><img src="/blog/images/Hdfs-arch1.png" alt="Hdfs架构组成"></p>
<p>由上图可以看出，NameNode的主备监控由ZKFC(ZKFailoverController)完成，ZKFC时独立运行的进程，每个ZKFC监控一个NameNode，当监控主节点的ZKFC发现主节点异常时，该ZKFC端开与ZooKeeper的连接，释放分布式锁，监控备用NameNode的ZKFC抢到锁并将其监控的Standby NameNode切换成Active NameNode。</p>
<p>ZooKeeper为ZKFC实现故障转移提供统一协调服务。通过ZooKeeper中watcher的监听机制，通知ZKFC异常NameNode下线并保证同一时刻只有一个Active NameNode。</p>
<h4 id="主备切换流程"><a href="#主备切换流程" class="headerlink" title="主备切换流程"></a>主备切换流程</h4><p>Hdfs集群刚启动时，NameNode节点状态都是Standby，之后每个NameNode节点都会启动ZKFC进程后去ZooKeeper集群抢占分布式锁，成功抢占分布式锁的NameNode会成为Active NameNode，之后ZKFC实时监控自己的NameNode。<br> Hdfs提供两种HA状态方式：<br> 一种是管理员运行命令 “DFSHAAdmin -failover”执行状态切换；另一种是自动切换。</p>
<ul>
<li><p>Active NameNode挂掉后，Standby NameNode如何升级？<br> Active NameNode挂掉后，对应的ZKFC进程检测到NameNode状态，向ZooKeeper发生删除锁的命令，锁删除后，触发一个事件回调Standby NameNode上的ZKFC。ZKFC收到消息后先去ZooKeeper抢锁，锁创建完成后会检查原来的主NameNode是否真的挂掉（排除其网络延迟等），确认挂掉则升级当前节点为Active NameNode；若没挂掉则先将原来的主节点降级为备节点，将当前节点升级为Active NameNode。</p>
</li>
<li><p>Active NameNode上的ZKFC进行挂掉而Active NameNode正常，如何切换？</p>
<p>ZKFC挂掉后，ZKFC与ZooKeeper之间的TCP连接断开，session消失锁被释放，触发事件回调Standby NameNode上的ZKFC进程，ZKFC去抢占锁，抢锁成功后与上述执行过程一致。</p>
</li>
</ul>
<h3 id="Hdfs读数据流程"><a href="#Hdfs读数据流程" class="headerlink" title="Hdfs读数据流程"></a>Hdfs读数据流程</h3><p><img src="/blog/images/Hdfs-read.png" alt="Hdfs读流程"></p>
<ol>
<li>打开文件：客户端调用*DistribuedFileSystem.open()<em>方法打开文件，该方法中调用</em>DFSclient.open()*方法得到DFSInputStream对象用于读取数据块</li>
<li>构造DFSInputStream：*DFScliernt.open()<em>构造DFSInputStream输出流时，调用</em>getBlockLocations()*方法向NameNode节点获取组成文件的block位置信息，且block的位置信息按与客户端距离的远近排序</li>
<li>连接DataNode读取数据块：客户端通过调用*DFSInputStream.read()<em>方法，连接到离客户端最近的一个DataNode读取block，数据会以数据包packet（注：client向DataNode或DataNode pipline之间传输数据的基本单位，默认64kb）为单位从DataNode通过流式接口传给客户端直到一个数据块读取完成；DFSInputStream会再次调用</em>getBlockLocations()*方法，获取下一个最优节点上数据块的位置</li>
<li>文件的所有block读取完成，调用close()方法，关闭输入流，释放资源</li>
</ol>
<h3 id="Hdfs写数据流程"><a href="#Hdfs写数据流程" class="headerlink" title="Hdfs写数据流程"></a>Hdfs写数据流程</h3><p><img src="/blog/images/Hdfs-write.png" alt="Hdfs写流程"></p>
<ol>
<li>在NameNode创建文件：当client写新文件时，调用*DistributedFileSystem.create()*方法，该方法中会生成一个DFSOutputStream对象，生成该对象时，会检查客户端是否已经打开；通过RPC与NameNode通信并创建新文件，构建出DFSOutputStream，该对象负责数据的写入</li>
<li>建立数据流pipeline管道：客户端得到一个输出流对象，通过调用*ClientProtocol.addBlock()*向NameNode申请新的空block，addBlock()会返回一个LocateBlock对象，该对象保存可写入的DataNode信息，然后构成pipeline</li>
<li>通过数据流pipeline写数据：当DFSOutputStream调用wirte()方法写入数据时，数据会先被缓存在一个缓冲区中，写入的数据会被切分为多个数据包，每达到一个数据包长度(65536Byte)时，DFSOutputStream会构造一个Packet对象保存当前要发送的数据包；新构造的packet对象则被放到DFSOutputStream维护的dataQueue队列中，DataStream线程会从dataQueue队列中取出packet对象，通过底层IO发送到pipeline中的第一个DataNode，然后第一个DataNode发送至第二个DataNode，以此类推。发送完成后该packet被移出dataQueue，并放入DFSOutputStream维护的确认队列ackQueue中，当收到所有DataNode的确认消息后，移出确认队列。(注：Chunk是Client向DataNode或者DataNode pipeline之间进行数据校验的基本单位，默认为512Byte，带有4Byte的校验位，因此实际上每个Chunk写入packet的大小为516Byte)</li>
<li>关闭输入流并提交文件：客户端完成整个文件中所有数据块block的写操作后，调用close()方法关闭输出流，然后调用*ClientProtoclo.complete()*方法通知NameNode提交该文件的所有block，NameNode确认该文件的备份数是否满足要求。</li>
</ol>

      
    </div>
</article>

    </li>
  
</ul>


            <footer>
    <div>© 2021 - Levi </div>
    <div>
        <span>
<!--            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>-->
            Homo sum, humani nihil a me alienum puto
        </span>
    </div>
<!--    {% if theme.footer.counter %}-->
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">Views <span id="busuanzi_value_site_pv"></span> </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Visitors <span id="busuanzi_value_site_uv"></span> </span>
<!--    {% endif %}-->
</footer>

        </div>
    </div>
</div>

<script src="/blog/js/pager/dist/singlepager.js"></script>

<script>
var sp = new Pager('data-pager-shell')

</script>
</body>
</html>